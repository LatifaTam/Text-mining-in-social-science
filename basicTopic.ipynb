{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1a0b09",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc80cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import set up\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "# remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data from Standford NLP lab, Merge, pre-processing\n",
    "https://data.stanford.edu/congress_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a432fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all speech and description data: only need this for once\n",
    "speech_full = pd.read_csv('hein-daily/hein-daily/speeches_106.txt',delimiter = '|',encoding='unicode_escape',on_bad_lines='skip')\n",
    "descr_full = pd.read_csv('hein-daily/hein-daily/descr_106.txt',delimiter = '|',encoding='unicode_escape',on_bad_lines='skip')\n",
    "\n",
    "for i in range(107, 115):\n",
    "    txt_name_sp = 'hein-daily/hein-daily/speeches_'+str(i)+'.txt'\n",
    "    cur_speech = pd.read_csv(txt_name_sp,delimiter = '|',encoding='unicode_escape',on_bad_lines='skip')\n",
    "    speech_full = speech_full.append(cur_speech)\n",
    "\n",
    "    txt_name_descr = 'hein-daily/hein-daily/descr_'+str(i)+'.txt'\n",
    "    cur_descr = pd.read_csv(txt_name_descr,delimiter = '|',encoding='unicode_escape',on_bad_lines='skip')\n",
    "    descr_full = descr_full.append(cur_descr)\n",
    "# Turn all id into intger\n",
    "descr_full['speech_id'] = descr_full['speech_id'].astype(int)\n",
    "speech_full['speech_id'] = speech_full['speech_id'].astype(int)\n",
    "# Turn time into timestamp format\n",
    "descr_full['date'] = pd.to_datetime(descr_full['date'].astype(str), format='%Y%m%d')\n",
    "# Merge speech with details\n",
    "full_data = speech_full.merge(descr_full, on='speech_id')\n",
    "# Raw speech 106-114th\n",
    "full_data.to_csv('fullData2000.csv',index=False) \n",
    "# No-stop-word speech 106-114th\n",
    "# Remove stop words from the 'speech' column\n",
    "stop_words = set(stopwords.words('english'))\n",
    "full_data['speech'] = full_data['speech'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "full_data.to_csv('fullData2000NoStop.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541bc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Raw data\n",
    "# fullData = pd.read_csv('fullData2000.csv')\n",
    "fullData = pd.read_csv('test50_labled.csv')\n",
    "\n",
    "# for data without stop words\n",
    "#fullData = pd.read_csv('fullData2000NoStop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba8d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "      <th>speech_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. President. I am honored to join my colleag...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_her_she_tax_he</td>\n",
       "      <td>['her', 'she', 'tax', 'he', 'his', 'budget', '...</td>\n",
       "      <td>her - she - tax - he - his - budget - medicare...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1060016994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But what about the gentlemans perfecting amend...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_her_she_tax_he</td>\n",
       "      <td>['her', 'she', 'tax', 'he', 'his', 'budget', '...</td>\n",
       "      <td>her - she - tax - he - his - budget - medicare...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1060093452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there process thatthe gentleman from Penn- ...</td>\n",
       "      <td>2786</td>\n",
       "      <td>2786_pennsylvania_tleman_debatewithout_firefig...</td>\n",
       "      <td>['pennsylvania', 'tleman', 'debatewithout', 'f...</td>\n",
       "      <td>pennsylvania - tleman - debatewithout - firefi...</td>\n",
       "      <td>0.128119</td>\n",
       "      <td>False</td>\n",
       "      <td>1060075797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ask unanimous consent that the resolution be...</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231_preamble_reconsider_agreed_laid</td>\n",
       "      <td>['preamble', 'reconsider', 'agreed', 'laid', '...</td>\n",
       "      <td>preamble - reconsider - agreed - laid - statem...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1060043252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. President. we will be doing wrapup momenta...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_her_she_tax_he</td>\n",
       "      <td>['her', 'she', 'tax', 'he', 'his', 'budget', '...</td>\n",
       "      <td>her - she - tax - he - his - budget - medicare...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1060087009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764065</th>\n",
       "      <td>Mr. Speaker. I rise today to honor the Borough...</td>\n",
       "      <td>737</td>\n",
       "      <td>737_township_town_settlers_village</td>\n",
       "      <td>['township', 'town', 'settlers', 'village', '2...</td>\n",
       "      <td>township - town - settlers - village - 250th -...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1140111484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764066</th>\n",
       "      <td>Mr. Chairman. I would like to thank Chairman C...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_her_she_tax_he</td>\n",
       "      <td>['her', 'she', 'tax', 'he', 'his', 'budget', '...</td>\n",
       "      <td>her - she - tax - he - his - budget - medicare...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>1140107676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764067</th>\n",
       "      <td>Pursuant to the order of the House of January ...</td>\n",
       "      <td>7383</td>\n",
       "      <td>7383_morninghour_lists_submitted_january</td>\n",
       "      <td>['morninghour', 'lists', 'submitted', 'january...</td>\n",
       "      <td>morninghour - lists - submitted - january - mi...</td>\n",
       "      <td>0.944563</td>\n",
       "      <td>False</td>\n",
       "      <td>1140081338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764068</th>\n",
       "      <td>The question is on the motion offered by the g...</td>\n",
       "      <td>972</td>\n",
       "      <td>972_suspend_offered_motion_texas</td>\n",
       "      <td>['suspend', 'offered', 'motion', 'texas', 'pas...</td>\n",
       "      <td>suspend - offered - motion - texas - pass - ru...</td>\n",
       "      <td>0.853863</td>\n",
       "      <td>False</td>\n",
       "      <td>1140102440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764069</th>\n",
       "      <td>Without objection. it is so ordered.</td>\n",
       "      <td>0</td>\n",
       "      <td>0_ordered_objection_without_so</td>\n",
       "      <td>['ordered', 'objection', 'without', 'so', 'it'...</td>\n",
       "      <td>ordered - objection - without - so - it - is -...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1140073457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>764070 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   speech  Topic  \\\n",
       "0       Mr. President. I am honored to join my colleag...     -1   \n",
       "1       But what about the gentlemans perfecting amend...     -1   \n",
       "2       Is there process thatthe gentleman from Penn- ...   2786   \n",
       "3       I ask unanimous consent that the resolution be...   1231   \n",
       "4       Mr. President. we will be doing wrapup momenta...     -1   \n",
       "...                                                   ...    ...   \n",
       "764065  Mr. Speaker. I rise today to honor the Borough...    737   \n",
       "764066  Mr. Chairman. I would like to thank Chairman C...     -1   \n",
       "764067  Pursuant to the order of the House of January ...   7383   \n",
       "764068  The question is on the motion offered by the g...    972   \n",
       "764069               Without objection. it is so ordered.      0   \n",
       "\n",
       "                                                     Name  \\\n",
       "0                                       -1_her_she_tax_he   \n",
       "1                                       -1_her_she_tax_he   \n",
       "2       2786_pennsylvania_tleman_debatewithout_firefig...   \n",
       "3                    1231_preamble_reconsider_agreed_laid   \n",
       "4                                       -1_her_she_tax_he   \n",
       "...                                                   ...   \n",
       "764065                 737_township_town_settlers_village   \n",
       "764066                                  -1_her_she_tax_he   \n",
       "764067           7383_morninghour_lists_submitted_january   \n",
       "764068                   972_suspend_offered_motion_texas   \n",
       "764069                     0_ordered_objection_without_so   \n",
       "\n",
       "                                           Representation  \\\n",
       "0       ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "1       ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "2       ['pennsylvania', 'tleman', 'debatewithout', 'f...   \n",
       "3       ['preamble', 'reconsider', 'agreed', 'laid', '...   \n",
       "4       ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "...                                                   ...   \n",
       "764065  ['township', 'town', 'settlers', 'village', '2...   \n",
       "764066  ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "764067  ['morninghour', 'lists', 'submitted', 'january...   \n",
       "764068  ['suspend', 'offered', 'motion', 'texas', 'pas...   \n",
       "764069  ['ordered', 'objection', 'without', 'so', 'it'...   \n",
       "\n",
       "                                              Top_n_words  Probability  \\\n",
       "0       her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "1       her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "2       pennsylvania - tleman - debatewithout - firefi...     0.128119   \n",
       "3       preamble - reconsider - agreed - laid - statem...     1.000000   \n",
       "4       her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "...                                                   ...          ...   \n",
       "764065  township - town - settlers - village - 250th -...     1.000000   \n",
       "764066  her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "764067  morninghour - lists - submitted - january - mi...     0.944563   \n",
       "764068  suspend - offered - motion - texas - pass - ru...     0.853863   \n",
       "764069  ordered - objection - without - so - it - is -...     1.000000   \n",
       "\n",
       "        Representative_document   speech_id  \n",
       "0                         False  1060016994  \n",
       "1                         False  1060093452  \n",
       "2                         False  1060075797  \n",
       "3                         False  1060043252  \n",
       "4                         False  1060087009  \n",
       "...                         ...         ...  \n",
       "764065                    False  1140111484  \n",
       "764066                    False  1140107676  \n",
       "764067                    False  1140081338  \n",
       "764068                    False  1140102440  \n",
       "764069                     True  1140073457  \n",
       "\n",
       "[764070 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477d770",
   "metadata": {},
   "source": [
    "### Stop word filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7c0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Apply the function to the 'speech' column\n",
    "fullData['speech'] = fullData['speech'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3c2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#full_data['speech'] = full_data['speech'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "newStop=['another','point','get','must','made','two','hr','say','go','come','much','may','make','even','going','also','would','could','think','chair','mr','madam','dont','said','did','does','thank','ask','today','just','000','gentleman','speaker','floor']\n",
    "for word in newStop:\n",
    "    stop_words.add(word)\n",
    "\n",
    "fullData['speech'] = fullData['speech'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "fullData.dropna(subset=['speech'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a4fc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                    speech  Topic  \\\n",
       "0       President honored join colleague Senior Senato...     -1   \n",
       "1                         gentlemans perfecting amendment     -1   \n",
       "2       process thatthe Penn cationdrug treatment nati...   2786   \n",
       "3       unanimous consent resolution agreed preamble a...   1231   \n",
       "4                            President wrapup momentarily     -1   \n",
       "...                                                   ...    ...   \n",
       "764065  rise honor Borough Shenandoah celebrated 150th...    737   \n",
       "764066  Chairman like Chairman CALVERT Ranking Member ...     -1   \n",
       "764067  Pursuant order House January 5 2016 recognize ...   7383   \n",
       "764068  question motion offered Texas House suspend ru...    972   \n",
       "764069                          Without objection ordered      0   \n",
       "\n",
       "                                                     Name  \\\n",
       "0                                       -1_her_she_tax_he   \n",
       "1                                       -1_her_she_tax_he   \n",
       "2       2786_pennsylvania_tleman_debatewithout_firefig...   \n",
       "3                    1231_preamble_reconsider_agreed_laid   \n",
       "4                                       -1_her_she_tax_he   \n",
       "...                                                   ...   \n",
       "764065                 737_township_town_settlers_village   \n",
       "764066                                  -1_her_she_tax_he   \n",
       "764067           7383_morninghour_lists_submitted_january   \n",
       "764068                   972_suspend_offered_motion_texas   \n",
       "764069                     0_ordered_objection_without_so   \n",
       "\n",
       "                                           Representation  \\\n",
       "0       ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "1       ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "2       ['pennsylvania', 'tleman', 'debatewithout', 'f...   \n",
       "3       ['preamble', 'reconsider', 'agreed', 'laid', '...   \n",
       "4       ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "...                                                   ...   \n",
       "764065  ['township', 'town', 'settlers', 'village', '2...   \n",
       "764066  ['her', 'she', 'tax', 'he', 'his', 'budget', '...   \n",
       "764067  ['morninghour', 'lists', 'submitted', 'january...   \n",
       "764068  ['suspend', 'offered', 'motion', 'texas', 'pas...   \n",
       "764069  ['ordered', 'objection', 'without', 'so', 'it'...   \n",
       "\n",
       "                                              Top_n_words  Probability  \\\n",
       "0       her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "1       her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "2       pennsylvania - tleman - debatewithout - firefi...     0.128119   \n",
       "3       preamble - reconsider - agreed - laid - statem...     1.000000   \n",
       "4       her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "...                                                   ...          ...   \n",
       "764065  township - town - settlers - village - 250th -...     1.000000   \n",
       "764066  her - she - tax - he - his - budget - medicare...     0.000000   \n",
       "764067  morninghour - lists - submitted - january - mi...     0.944563   \n",
       "764068  suspend - offered - motion - texas - pass - ru...     0.853863   \n",
       "764069  ordered - objection - without - so - it - is -...     1.000000   \n",
       "\n",
       "        Representative_document   speech_id  \n",
       "0                         False  1060016994  \n",
       "1                         False  1060093452  \n",
       "2                         False  1060075797  \n",
       "3                         False  1060043252  \n",
       "4                         False  1060087009  \n",
       "...                         ...         ...  \n",
       "764065                    False  1140111484  \n",
       "764066                    False  1140107676  \n",
       "764067                    False  1140081338  \n",
       "764068                    False  1140102440  \n",
       "764069                     True  1140073457  \n",
       "\n",
       "[764070 rows x 8 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullData.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842b44e",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568bd875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('test50.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76b758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(group_train)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_9740\\4141864504.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_df = eval_df.append(group_eval)\n"
     ]
    }
   ],
   "source": [
    "# split data into 80-20 for training- evaluate\n",
    "# see if same things workds well everywhere\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df = pd.DataFrame()\n",
    "eval_df = pd.DataFrame()\n",
    "\n",
    "fullData['date'] = pd.to_datetime(fullData['date'])\n",
    "# Shuffle the DataFrame to ensure randomness\n",
    "shuffled_df = fullData.sample(frac=1, random_state=42)\n",
    "\n",
    "# Group by 'Year' and split 80-20 groups\n",
    "shuffled_df['Year'] = shuffled_df['date'].dt.year\n",
    "groups = shuffled_df.groupby('Year')\n",
    "\n",
    "for _, group in groups:\n",
    "    group_train, group_eval = train_test_split(group, test_size=0.5, random_state=42)\n",
    "    train_df = train_df.append(group_train)\n",
    "    eval_df = eval_df.append(group_eval)\n",
    "\n",
    "# Reset the index for the final DataFrames\n",
    "train_df.reset_index(drop=True, inplace=True) # 80%\n",
    "eval_df.reset_index(drop=True, inplace=True) # 20%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0463c4",
   "metadata": {},
   "source": [
    "### Word Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWord = pd.read_csv('phrase_clusters/keywords.txt',delimiter = '|',encoding='unicode_escape',on_bad_lines='skip')\n",
    "environment_keyWord = list(keyWord[keyWord['topic']=='environment']['phrase'])\n",
    "environment_keyWord_freq = [0]*len(environment_keyWord)\n",
    "fullData['SFkeyCount']= 0\n",
    "\n",
    "for index, row in fullData.iterrows():\n",
    "    speech_text = row['speech']\n",
    "    count=0\n",
    "    for wc,word in enumerate(environment_keyWord):\n",
    "        kw_count = speech_text.lower().count(word)\n",
    "        count = count + kw_count\n",
    "        environment_keyWord_freq[wc] = environment_keyWord_freq[wc] + kw_count\n",
    "    #save count\n",
    "    fullData.at[index, 'SFkeyCount'] = count\n",
    "fullData['SFkeyFreq'] = fullData['SFkeyCount'] / fullData['word_count']\n",
    "\n",
    "# save keyword counting to csv\n",
    "fullData.to_csv('fullDataKw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe our new col\n",
    "pd.set_option('display.float_format', '{:.2f}'.format) # normal description: not scientific \n",
    "fullData[['SFkeyFreq','word_count','SFkeyCount']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count frequency for all keywords\n",
    "def merge(list1, list2):\n",
    " \n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "     \n",
    "    return merged_list\n",
    "merge(environment_keyWord_freq,environment_keyWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_t =[value for value in fullData['SFkeyCount'] if value > 5]\n",
    "\n",
    "plt.hist(list_t, bins=10, edgecolor='k')  # You can adjust the number of bins as needed\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Column A')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg = cleaned_df.groupby(pd.PeriodIndex(cleaned_df['date'], freq=\"M\"))['word_count'].mean().to_frame().reset_index()\n",
    "monthly_avg.plot(x='date', y='word_count', kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865162ad",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96993ee",
   "metadata": {},
   "source": [
    "### Dynamic Topic modeling: Bertopic\n",
    "https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html#visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY USE 3.11 for this\n",
    "# you might want to use seperate environment for Bert and sklearn, as they have different version requirement for some common dependencies\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small sample set here cuz Bertopic takes forever\n",
    "test_df = fullData[fullData['speech_id'] < 1080000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bert and handle overtime changes\n",
    "bert_model = BERTopic(verbose=True)\n",
    "topics, _ = bert_model.fit_transform(test_df['speech'])\n",
    "topics_over_time = bert_model.topics_over_time(test_df['speech'], test_df['date'], nr_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c77df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79195c23",
   "metadata": {},
   "source": [
    "### LDA and NMF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264436f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use conda python3.9 for this\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5444f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "tf = tf_vectorizer.fit_transform(fullData['speech'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0,n_components=30).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17548948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 29, 17, ..., 27, 26, 15], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA label \n",
    "most_dominant_topics = lda.transform(tf).argmax(axis=1)\n",
    "\n",
    "# Add the most dominant topic information as a new column to your DataFrame\n",
    "#df['Most_Dominant_Topic'] = most_dominant_topics\n",
    "\n",
    "# Display the updated DataFrame\n",
    "most_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d562bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanay\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# NMF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(fullData['speech'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "nmf = NMF(n_components=30, random_state=1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feec2099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  6, 20, ..., 28, 14,  1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NMF label\n",
    "most_dominant_topics_NMF = nmf.transform(tfidf).argmax(axis=1)\n",
    "\n",
    "# Add the most dominant topic information as a new column to your DataFrame\n",
    "#df['Most_Dominant_Topic'] = most_dominant_topics\n",
    "\n",
    "# Display the updated DataFrame\n",
    "most_dominant_topics_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f004eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData.to_csv('test50_labled_LDA_NMF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7d3e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 111.78032030480962\n"
     ]
    }
   ],
   "source": [
    "perplexity = lda.perplexity(tf)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50bd9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "president consent call unanimous order time 10 office number senate\n",
      "Topic 1:\n",
      "one want know us people time country way work right\n",
      "Topic 2:\n",
      "program veterans funding programs million department support provide services year\n",
      "Topic 3:\n",
      "yield important hope home high help health great government good\n",
      "Topic 4:\n",
      "business small 10 next day time use first president senate\n",
      "Topic 5:\n",
      "yield minutes california time new chairman 10 member committee like\n",
      "Topic 6:\n",
      "senate resolution question majority committee process passed president republican 10\n",
      "Topic 7:\n",
      "jobs percent economy million economic new year years work need\n",
      "Topic 8:\n",
      "people american money government americans need pay like want world\n",
      "Topic 9:\n",
      "energy rights right continue americans act future nation us nations\n",
      "Topic 10:\n",
      "families children family home day help every many life support\n",
      "Topic 11:\n",
      "health care services americans system act percent provide need pay\n",
      "Topic 12:\n",
      "budget security spending billion year money republican years president congress\n",
      "Topic 13:\n",
      "yield important hope home high help health great government good\n",
      "Topic 14:\n",
      "war military iraq world us nation security american president nations\n",
      "Topic 15:\n",
      "rule objection without order house minutes time first continue number\n",
      "Topic 16:\n",
      "national women support rise colleagues year members services every first\n",
      "Topic 17:\n",
      "president law congress administration policy us last new years act\n",
      "Topic 18:\n",
      "committee chairman member work time like members leadership want support\n",
      "Topic 19:\n",
      "states united us world america nations country state support nation\n",
      "Topic 20:\n",
      "federal state government public department local office law one congress\n",
      "Topic 21:\n",
      "school education high children local state public programs year program\n",
      "Topic 22:\n",
      "tax pay families increase americans year federal money economy working\n",
      "Topic 23:\n",
      "debate system process issue important first believe continue opportunity fact\n",
      "Topic 24:\n",
      "senator time yield president back number right republican senate colleagues\n",
      "Topic 25:\n",
      "community years service many district life new work first great\n",
      "Topic 26:\n",
      "bill legislation act support passed important colleagues time senate help\n",
      "Topic 27:\n",
      "house vote members congress passed majority 10 opportunity committee time\n",
      "Topic 28:\n",
      "report record part 10 time president new provide without debate\n",
      "Topic 29:\n",
      "amendment chairman question part time bill president committee senate passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display LDA result\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "861dbe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "people, us, one, want, know, american, country, right, like, way\n",
      "\n",
      "Topic 2:\n",
      "objection, next, day, america, continue, republican, see, district, united, without\n",
      "\n",
      "Topic 3:\n",
      "senator, right, 10, state, debate, issue, know, believe, work, see\n",
      "\n",
      "Topic 4:\n",
      "yield, back, member, good, like, legislation, colleagues, support, great, district\n",
      "\n",
      "Topic 5:\n",
      "consent, unanimous, members, record, 10, debate, passed, committee, office, services\n",
      "\n",
      "Topic 6:\n",
      "minutes, 10, policy, member, debate, like, last, yield, rise, take\n",
      "\n",
      "Topic 7:\n",
      "amendment, debate, support, rise, believe, colleagues, part, right, back, program\n",
      "\n",
      "Topic 8:\n",
      "time, back, continue, debate, leadership, passed, like, support, use, first\n",
      "\n",
      "Topic 9:\n",
      "vote, members, debate, record, act, colleagues, passed, first, district, republican\n",
      "\n",
      "Topic 10:\n",
      "bill, legislation, passed, act, support, important, members, provide, record, debate\n",
      "\n",
      "Topic 11:\n",
      "call, consent, unanimous, number, district, like, home, family, day, nation\n",
      "\n",
      "Topic 12:\n",
      "chairman, rise, last, support, want, like, work, number, funding, issue\n",
      "\n",
      "Topic 13:\n",
      "order, leadership, debate, member, budget, part, continue, law, first, state\n",
      "\n",
      "Topic 14:\n",
      "president, first, administration, right, believe, office, iraq, law, war, next\n",
      "\n",
      "Topic 15:\n",
      "question, back, debate, put, one, bill, district, consent, let, know\n",
      "\n",
      "Topic 16:\n",
      "house, members, last, congress, republican, passed, part, district, public, day\n",
      "\n",
      "Topic 17:\n",
      "rule, last, debate, law, record, first, state, process, district, act\n",
      "\n",
      "Topic 18:\n",
      "california, state, member, district, want, like, good, energy, million, local\n",
      "\n",
      "Topic 19:\n",
      "new, energy, state, first, economic, small, jobs, legislation, like, including\n",
      "\n",
      "Topic 20:\n",
      "report, department, part, year, security, administration, military, congress, number, services\n",
      "\n",
      "Topic 21:\n",
      "senate, office, 10, debate, public, passed, day, committee, republican, use\n",
      "\n",
      "Topic 22:\n",
      "tax, budget, billion, year, percent, million, federal, program, money, security\n",
      "\n",
      "Topic 23:\n",
      "states, united, act, state, support, national, law, legislation, congress, federal\n",
      "\n",
      "Topic 24:\n",
      "health, care, act, veterans, services, children, legislation, women, americans, provide\n",
      "\n",
      "Topic 25:\n",
      "committee, member, services, energy, office, security, legislation, congress, public, process\n",
      "\n",
      "Topic 26:\n",
      "resolution, record, debate, support, budget, national, members, colleagues, war, day\n",
      "\n",
      "Topic 27:\n",
      "without, record, passed, right, congress, debate, see, continue, cannot, public\n",
      "\n",
      "Topic 28:\n",
      "business, small, 10, jobs, economic, economy, use, day, community, act\n",
      "\n",
      "Topic 29:\n",
      "majority, members, policy, debate, republican, legislation, first, congress, process, next\n",
      "\n",
      "Topic 30:\n",
      "community, service, school, years, rise, state, family, life, many, education\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the NMF components GPT\n",
    "nmf_components = nmf.components_\n",
    "\n",
    "# Number of top words to print for each topic\n",
    "top_n_words = 10\n",
    "\n",
    "# Loop through each topic\n",
    "for topic_idx, topic in enumerate(nmf_components):\n",
    "    # Get the indices of the top N words for the current topic\n",
    "    top_word_indices = topic.argsort()[-top_n_words:][::-1]\n",
    "\n",
    "    # Get the actual words from the vocabulary\n",
    "    top_words = [tfidf_feature_names[i] for i in top_word_indices]\n",
    "\n",
    "    # Print the topic number and top words\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print(\", \".join(top_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom in 2008 note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "fullData = pd.read_csv('fullData2000.csv')\n",
    "#2008 only \n",
    "fullData['date'] = pd.to_datetime(fullData['date'])\n",
    "data_2008 = fullData[fullData['date'].dt.year == 2008]\n",
    "result_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "def colocated(target_word, fullData,result,year):\n",
    "    fullData['tokens'] = fullData['speech'].apply(word_tokenize)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    newStop=['president','object','another','point','get','must','made','two','hr','say','go','come','much','may','make','even','going','also','would','could','think','chair','mr','madam','dont','said','did','does','thank','ask','today','just','000','gentleman','speaker','floor']\n",
    "    for word in newStop:\n",
    "        stop_words.add(word)\n",
    "    fullData['filtered_tokens'] = fullData['tokens'].apply(lambda x: [word.lower() for word in x if word.isalpha() and word.lower() not in stop_words])\n",
    "\n",
    "    fullData['bigrams'] = fullData['filtered_tokens'].apply(lambda x: list(bigrams(x)))\n",
    "    all_bigrams = [item for sublist in fullData['bigrams'] for item in sublist]\n",
    "    bigram_counts = Counter(all_bigrams)\n",
    "    co_occurring_bigrams = [bigram for bigram in bigram_counts if target_word in bigram][:10]\n",
    "    \n",
    "    # Store the co-located words (excluding the target word) in lists\n",
    "    co_located_bigram_words = []\n",
    "    for bigram in co_occurring_bigrams:\n",
    "        co_located_bigram_words.extend([word for word in bigram if word != target_word])\n",
    "\n",
    "    # most common\n",
    "    co_occurring_bigrams_common = bigram_counts.most_common(15)\n",
    "\n",
    "    # Print the stored lists\n",
    "    print(\"Co-located Bigram Words without '{}':\".format(target_word), co_located_bigram_words)\n",
    "    print(\"Co-located Bigram Words in common\" , co_occurring_bigrams_common)\n",
    "\n",
    "    \n",
    "    result_df[year] = co_located_bigram_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_16952\\1389264362.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fullData['tokens'] = fullData['speech'].apply(word_tokenize)\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_16952\\1389264362.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fullData['filtered_tokens'] = fullData['tokens'].apply(lambda x: [word.lower() for word in x if word.isalpha() and word.lower() not in stop_words])\n",
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_16952\\1389264362.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fullData['bigrams'] = fullData['filtered_tokens'].apply(lambda x: list(bigrams(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-located Bigram Words without 'energy': ['time', 'public', 'committee', 'commerce', 'meeting', 'executives', 'saying', 'important', 'leaders', 'white']\n",
      "Co-located Bigram Words in common [(('united', 'states'), 16821), (('health', 'care'), 13945), (('american', 'people'), 8390), (('unanimous', 'consent'), 7968), (('small', 'businesses'), 5959), (('small', 'business'), 5145), (('balance', 'time'), 4991), (('without', 'objection'), 4887), (('wall', 'street'), 4668), (('objection', 'ordered'), 4481), (('federal', 'government'), 4363), (('new', 'york'), 4281), (('urge', 'colleagues'), 4171), (('health', 'insurance'), 3562), (('men', 'women'), 3153)]\n"
     ]
    }
   ],
   "source": [
    "year = 2010\n",
    "data = fullData[fullData['date'].dt.year == year]\n",
    "colocated('energy', data,result_df,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2008</th>\n",
       "      <th>2007</th>\n",
       "      <th>2006</th>\n",
       "      <th>2009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>committee</td>\n",
       "      <td>squeezed</td>\n",
       "      <td>department</td>\n",
       "      <td>money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commerce</td>\n",
       "      <td>policy</td>\n",
       "      <td>prescribe</td>\n",
       "      <td>every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assignment</td>\n",
       "      <td>education</td>\n",
       "      <td>amendments</td>\n",
       "      <td>finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>less</td>\n",
       "      <td>begin</td>\n",
       "      <td>policy</td>\n",
       "      <td>breathe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heat</td>\n",
       "      <td>promote</td>\n",
       "      <td>declares</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time</td>\n",
       "      <td>independence</td>\n",
       "      <td>independence</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loved</td>\n",
       "      <td>countrys</td>\n",
       "      <td>renewable</td>\n",
       "      <td>wind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>difference</td>\n",
       "      <td>solve</td>\n",
       "      <td>combats</td>\n",
       "      <td>investments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nitro</td>\n",
       "      <td>crisis</td>\n",
       "      <td>comes</td>\n",
       "      <td>clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>independent</td>\n",
       "      <td>renewable</td>\n",
       "      <td>hightechnological</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2008          2007               2006         2009\n",
       "0    committee      squeezed         department        money\n",
       "1     commerce        policy          prescribe        every\n",
       "2   assignment     education         amendments      finding\n",
       "3         less         begin             policy      breathe\n",
       "4         heat       promote           declares         need\n",
       "5         time  independence       independence     security\n",
       "6        loved      countrys          renewable         wind\n",
       "7   difference         solve            combats  investments\n",
       "8        nitro        crisis              comes        clean\n",
       "9  independent     renewable  hightechnological           NA"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
