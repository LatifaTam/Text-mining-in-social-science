{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims on cross-compare and double check accuracy and precision of topic modeling label. </p>\n",
    "Main steps in this notebook:</p>\n",
    "1) pull up representative speeches labled by each model about environment;</p>\n",
    "2) cross-check the results from each model: do they share a lot common? Do all models label the same group of speech as environment-relevant? </p>\n",
    "3) Mannual check precision and accuracy for randomly selected speeches.</p>\n",
    "4) Explore collocated-words in interested speeches.</p>\n",
    "5) merge speeches with speech information (like speakers information) and output environment speeches and data for peer lab member to further investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find topic with Standford keywords\n",
    "# environment: 3/ 520\n",
    "# sd 406: https://www.sanders.senate.gov/press-releases/sanders-boxer-to-introduce-major-climate-change-legislation-2/\n",
    "envir_topic_bert = []\n",
    "for topicIndex in topics:\n",
    "    for pair in topics[topicIndex]:\n",
    "        for i in range(len(pair)):\n",
    "            for word in environment_keyWord:\n",
    "                if word == pair[i]:\n",
    "                    envir_topic_bert.append(topicIndex)\n",
    "                    print(topicIndex)  # each key words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fullData = pd.read_csv('test50_labled.csv')\n",
    "bert_kw_List = [3,2761, 4476, 2343, 4294, 7408, 43,110,115,595,672,849,1311,2643,2805,2855,3184,3878,4148,4690,4821,4887,5892,6154,6159,6228,6532,6675,6820]\n",
    "lda_kw_List = [9]\n",
    "nmf_kw_List = [18,19,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('test50.csv')\n",
    "fullData = pd.merge(fullData, new_data[['speech_id', 'date']], on='speech_id', how='left')\n",
    "fullData = pd.merge(fullData, new_data[['speech_id', 'Year']], on='speech_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from collections import Counter\n",
    "fullDataBack =fullData\n",
    "fullData['speech'] = fullData['speech'].str.split().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(word_list):\n",
    "    if isinstance(word_list, list):\n",
    "        return [wn.morphy(word) for word in word_list]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply the lemmatization function to the 'speech' column\n",
    "fullData['lemmas'] = fullData['speech'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(s):\n",
    "    try:\n",
    "        # Try to evaluate the string as a Python literal\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If there's an error, return an empty list\n",
    "        return []\n",
    "\n",
    "# Apply the conversion function to the 'speech' column\n",
    "fullData['speech'] = fullData['speech'].apply(convert_to_list)\n",
    "\n",
    "# Filter out rows containing the word 'energy'\n",
    "filtered_data = fullData[fullData['speech'].apply(lambda x: 'oil' in x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fullData = fullData[fullData[\"speech\"].contains('energy')]\n",
    "#fullData['lemmas'] = [[wn.morphy(word) for word in row] for row in fullData['speech']]\n",
    "fullData=filtered_data\n",
    "fullData['lemmas'] = [[word for word in row if word is not None] for row in fullData['lemmas']] # for each word of each row in 'lemmas', keep anything that isn't 'None' type.\n",
    "#fullData['lemmas'] = [' '.join(row) for row in fullData['lemmas']] # glue the individual list of lemmas back into one string per speech\n",
    "\n",
    "top_words_per_year_lemmatized = {}\n",
    "\n",
    "for year in fullData['Year'].unique():\n",
    "    yearly_lemmas = []\n",
    "    for lemmas in fullData[fullData['Year'] == year]['lemmas']:\n",
    "        yearly_lemmas.extend(lemmas)\n",
    "\n",
    "    # Calculate frequency\n",
    "    lemma_counts = Counter(yearly_lemmas)\n",
    "\n",
    "    # Get the 10 most common lemmas\n",
    "    top_lemmas = lemma_counts.most_common(10)\n",
    "    top_words_per_year_lemmatized[year] = top_lemmas\n",
    "\n",
    "top_words_per_year_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData['speech_id'].describe()\n",
    "#1060000000 to 1140000000\n",
    "filtered_data = fullData[(fullData['speech_id'] >= 1060000000) & (fullData['speech_id'] < 1070000000)]['speech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_context_count = fullData[\"speech\"].str.split().explode().dropna().value_counts()\n",
    "year1_context_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "fullData[fullData['Topic']==2343]['speech'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out each topic keywords with an example\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "for num in bert_kw_List:\n",
    "    print(num)\n",
    "    print(fullData[fullData['Topic']==num]['Representation'].head(1))\n",
    "    print(fullData[fullData['Topic']==num]['speech'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sp_List = set()\n",
    "lda_sp_List = set()\n",
    "nmf_sp_List = set()\n",
    "check_id = 0\n",
    "\n",
    "for i in range(len(fullData)):\n",
    "    check_id=check_id+1\n",
    "    id=fullData['speech_id'][i]\n",
    "    for bkw in bert_kw_List:\n",
    "        if fullData['Topic'][i] == bkw:\n",
    "            bert_sp_List.add(id)\n",
    "    for lkw in lda_kw_List:\n",
    "        if fullData['Most_Dominant_Topic_LDA'][i] == lkw:\n",
    "            lda_sp_List.add(id)\n",
    "    for nkw in nmf_kw_List:\n",
    "        if fullData['Most_Dominant_Topic_NMF'][i] == nkw:\n",
    "            nmf_sp_List.add(id)\n",
    "\n",
    "#bert_sp_List = set(bert_sp_List)\n",
    "#lda_sp_List = set(lda_sp_List)\n",
    "#nmf_sp_List = set(nmf_sp_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocated words for top keywords\n",
    "before and after 2008 and 2001\n",
    "    some readings\n",
    "check out the pattern/ trend for minor topic ( without topic 3)\n",
    "sentimental analysis\n",
    "    tidytext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7810\n",
      "yes\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "bert_kw_List = [3,2761, 4476, 2343, 7408, 43,115,595,672,1311,2643,2805,2855,3184,4148,4690,4821,4887,5892,6154,6159,6532,6675,6820]\n",
    "#df = pd.read_csv('gpt4_5000_result.csv')\n",
    "\n",
    "\n",
    "bert_sp_List = set()\n",
    "check_id = 0\n",
    "\n",
    "for i in range(len(fullData)):\n",
    "    id=fullData['speech_id'][i]\n",
    "    for bkw in bert_kw_List:\n",
    "        if fullData['Topic'][i] == bkw:\n",
    "            check_id=check_id+1\n",
    "            bert_sp_List.add(id)\n",
    "\n",
    "#bert_sp_List = set(bert_sp_List)\n",
    "print(check_id)\n",
    "common_speech_ids = set(df['speech_id']).intersection(bert_sp_List)\n",
    "print(\"yes\")\n",
    "print(len(common_speech_ids))\n",
    "\n",
    "\n",
    "# Convert set to a Pandas DataFrame\n",
    "#df = pd.DataFrame({'set_column': [bert_sp_List]})\n",
    "\n",
    "# Specify the CSV file path\n",
    "#csv_file_path = 'random50EnvironmentSpeechid.csv'\n",
    "\n",
    "# Write to CSV file\n",
    "#df.to_csv(csv_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whatever into pickle\n",
    "pickle_file_path = 'bert_environ.pkl'\n",
    "\n",
    "# Open the pickle file for writing in binary mode ('wb')\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    # Dump the set into the pickle file\n",
    "    pickle.dump(common_speech_ids, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Louis pickle and take out mannual annotated \"yes\"\n",
    "import pickle\n",
    "\n",
    "# Specify the path to your pickle file\n",
    "pickle_file_path = 'manualAnnotation.pkl'\n",
    "\n",
    "# Open the pickle file for reading in binary mode ('rb')\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    result35 = pickle.load(file)\n",
    "# mannual annotate\n",
    "keys_with_y_value = {key for key, value in result35.items() if value == 'y'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# compare set of \"yes\" from various models\n",
    "#87 vs 72: 22 in common\n",
    "common_speech_ids_bert_35 = keys_with_y_value.intersection(common_speech_ids)\n",
    "print(len(common_speech_ids_bert_35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert set to Pandas DataFrame\n",
    "import random\n",
    "sampled_items = random.sample(sorted(bert_sp_List), 100)\n",
    "\n",
    "df = pd.DataFrame({'set_column': [sampled_items]})\n",
    "\n",
    "# Explode the set into separate rows\n",
    "df_exploded = df['set_column'].explode().reset_index(drop=True).to_frame()\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file_path = 'random50EnvironmentSpeechid_100.csv'\n",
    "\n",
    "# Write to CSV file\n",
    "df_exploded.to_csv(csv_file_path, index=False, header=['set_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-check environment topic from bert\n",
    "fullData = pd.read_csv('test50_labled.csv')\n",
    "bert_kw_List = [3,2761, 4476, 2343, 4294, 7408, 43,110,115,595,672,849,1311,2643,2805,2855,3184,3878,4148,4690,4821,4887,5892,6154,6159,6228,6532,6675,6820]\n",
    "\n",
    "filtered_data = fullData[\n",
    "    fullData.apply(lambda row: row['Representative_document'] and row['Topic'] in bert_kw_List, axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json for bertopic keyword and topic dictionary\n",
    "import json\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'Bertopic_50_dic.json'\n",
    "\n",
    "# Open the JSON file for reading\n",
    "with open(json_file_path, 'r') as file:\n",
    "    # Load the JSON data from the file\n",
    "    bertKeyWordic = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about Bertopic result\n",
    "bert_kw_List = [3,2761, 4476, 2343, 7408, 43,115,595,672,1311,2643,2805,2855,3184,4148,4690,4821,4887,5892,6154,6159,6532,6675,6820]\n",
    "\n",
    "previously wrong from standford keyword: 110( the word \"object\"), 4294('na'), 6228('include location Penn'), 849( include California), 3878(texas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_kw_List = [3,2761, 4476, 2343, 7408, 43,115,595,672,1311,2643,2805,2855,3184,4148,4690,4821,4887,5892,6154,6159,6532,6675,6820]\n",
    "filtered_data_only_correct_en = fullData[\n",
    "    fullData.apply(lambda row: row['Topic'] in bert_kw_List, axis=1)\n",
    "]\n",
    "filtered_data_only_correct_en.to_csv(\"Environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['speech','chamber','date','speaker','first_name','last_name','state','gender','char_count','word_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all demographic information for speeches\n",
    "# with full demo data\n",
    "fullData = pd.read_csv('fullData2000.csv')\n",
    "# join topic and keywords\n",
    "lables = pd.read_csv('test50_labled.csv')\n",
    "label_fullSpeech_demo = pd.merge(lables[['Topic', 'Representation','Representative_document','speech_id']], \n",
    "                                 fullData[['speech_id','speech','chamber','date','speaker','first_name','last_name','state','gender','char_count','word_count']],\n",
    "                                 on='speech_id', how='left')\n",
    "\n",
    "# filter and only keep environment relevant topics\n",
    "bert_kw_List = [3,2761, 4476, 2343, 7408, 43,115,595,672,1311,2643,2805,2855,3184,4148,4690,4821,4887,5892,6154,6159,6532,6675,6820]\n",
    "filtered_data_demo = label_fullSpeech_demo[label_fullSpeech_demo.apply(lambda row: row['Topic'] in bert_kw_List, axis=1)]\n",
    "#save to csv\n",
    "filtered_data_demo.to_csv(\"EnvironmentSpeech_WithInfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data\n",
    "fullData = pd.read_csv('fullData2000.csv')\n",
    "\n",
    "result_df = pd.merge(filtered_data, fullData[['speech_id', 'speech']], on='speech_id', how='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
